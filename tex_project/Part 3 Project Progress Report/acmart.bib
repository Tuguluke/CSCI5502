
@misc{ji_win-win_2022,
	title = {Win-{Win} {Cooperation}: {Bundling} {Sequence} and {Span} {Models} for {Named} {Entity} {Recognition}},
	shorttitle = {Win-{Win} {Cooperation}},
	url = {http://arxiv.org/abs/2207.03300},
	abstract = {For Named Entity Recognition (NER), sequence labeling-based and span-based paradigms are quite different. Previous research has demonstrated that the two paradigms have clear complementary advantages, but few models have attempted to leverage these advantages in a single NER model as far as we know. In our previous work, we proposed a paradigm known as Bundling Learning (BL) to address the above problem. The BL paradigm bundles the two NER paradigms, enabling NER models to jointly tune their parameters by weighted summing each paradigm's training loss. However, three critical issues remain unresolved: When does BL work? Why does BL work? Can BL enhance the existing state-of-the-art (SOTA) NER models? To address the first two issues, we implement three NER models, involving a sequence labeling-based model--SeqNER, a span-based NER model--SpanNER, and BL-NER that bundles SeqNER and SpanNER together. We draw two conclusions regarding the two issues based on the experimental results on eleven NER datasets from five domains. We then apply BL to five existing SOTA NER models to investigate the third issue, consisting of three sequence labeling-based models and two span-based models. Experimental results indicate that BL consistently enhances their performance, suggesting that it is possible to construct a new SOTA NER system by incorporating BL into the current SOTA system. Moreover, we find that BL reduces both entity boundary and type prediction errors. In addition, we compare two commonly used labeling tagging methods as well as three types of span semantic representations.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Ji, Bin and Li, Shasha and Yu, Jie and Ma, Jun and Liu, Huijun},
	month = jul,
	year = {2022},
	note = {arXiv:2207.03300 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/UYI22EYA/Ji et al. - 2022 - Win-Win Cooperation Bundling Sequence and Span Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/LEJ3ADZU/2207.html:text/html},
}

@misc{smirnova_evaluation_2022,
	title = {Evaluation of {Embedding} {Models} for {Automatic} {Extraction} and {Classification} of {Acknowledged} {Entities} in {Scientific} {Documents}},
	url = {http://arxiv.org/abs/2206.10939},
	abstract = {Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the Flair NLP-framework. The training was conducted using three default Flair NER models with two differently-sized corpora. The Flair Embeddings model trained on the larger training corpus showed the best accuracy of 0.77. Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation and miscellaneous. The model works more precise for some entity types than the others, thus, individuals and grant numbers showed very good F1-Score over 0.9. Most of the previous works on acknowledgement analysis were limited by the manual evaluation of data and therefore by the amount of processed data. This model can be applied for the comprehensive analysis of the acknowledgement texts and may potentially make a great contribution to the field of automated acknowledgement analysis.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Smirnova, Nina and Mayr, Philipp},
	month = jun,
	year = {2022},
	note = {arXiv:2206.10939 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Digital Libraries},
	annote = {Comment: Accepted workshop paper at EEKE2022 Workshop(JCDL2022)},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/FA3WWBX9/Smirnova and Mayr - 2022 - Evaluation of Embedding Models for Automatic Extra.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/9P5YL7DB/2206.html:text/html},
}

@misc{zhou_deep_2022,
	title = {Deep {Clustering} with {Features} from {Self}-{Supervised} {Pretraining}},
	url = {http://arxiv.org/abs/2207.13364},
	abstract = {A deep clustering model conceptually consists of a feature extractor that maps data points to a latent space, and a clustering head that groups data points into clusters in the latent space. Although the two components used to be trained jointly in an end-to-end fashion, recent works have proved it beneficial to train them separately in two stages. In the first stage, the feature extractor is trained via self-supervised learning, which enables the preservation of the cluster structures among the data points. To preserve the cluster structures even better, we propose to replace the first stage with another model that is pretrained on a much larger dataset via self-supervised learning. The method is simple and might suffer from domain shift. Nonetheless, we have empirically shown that it can achieve superior clustering performance. When a vision transformer (ViT) architecture is used for feature extraction, our method has achieved clustering accuracy 94.0\%, 55.6\% and 97.9\% on CIFAR-10, CIFAR-100 and STL-10 respectively. The corresponding previous state-of-the-art results are 84.3\%, 47.7\% and 80.8\%. Our code will be available online with the publication of the paper.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Zhou, Xingzhi and Zhang, Nevin L.},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13364 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/T88JZ2TH/Zhou and Zhang - 2022 - Deep Clustering with Features from Self-Supervised.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/6XSFI2ZX/2207.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/K9A3JL6T/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/M7PYHB6I/1706.html:text/html},
}

@misc{decorte_jobbert_2021,
	title = {{JobBERT}: {Understanding} {Job} {Titles} through {Skills}},
	shorttitle = {{JobBERT}},
	url = {http://arxiv.org/abs/2109.09605},
	abstract = {Job titles form a cornerstone of today's human resources (HR) processes. Within online recruitment, they allow candidates to understand the contents of a vacancy at a glance, while internal HR departments use them to organize and structure many of their processes. As job titles are a compact, convenient, and readily available data source, modeling them with high accuracy can greatly benefit many HR tech applications. In this paper, we propose a neural representation model for job titles, by augmenting a pre-trained language model with co-occurrence information from skill labels extracted from vacancies. Our JobBERT method leads to considerable improvements compared to using generic sentence encoders, for the task of job title normalization, for which we release a new evaluation benchmark.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Decorte, Jens-Joris and Van Hautte, Jeroen and Demeester, Thomas and Develder, Chris},
	month = sep,
	year = {2021},
	note = {arXiv:2109.09605 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to the International workshop on Fair, Effective And Sustainable Talent management using data science (FEAST) as part of ECML-PKDD 2021},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/DRC5LXB7/Decorte et al. - 2021 - JobBERT Understanding Job Titles through Skills.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/ZEFYX29Y/2109.html:text/html},
}

@misc{breidenbach_implicit_2021,
	title = {Implicit {Gender} {Bias} in {Computer} {Science} -- {A} {Qualitative} {Study}},
	url = {http://arxiv.org/abs/2107.01624},
	abstract = {Gender diversity in the tech sector is - not yet? - sufficient to create a balanced ratio of men and women. For many women, access to computer science is hampered by socialization-related, social, cultural and structural obstacles. The so-called implicit gender bias has a great influence in this respect. The lack of contact in areas of computer science makes it difficult to develop or expand potential interests. Female role models as well as more transparency of the job description should help women to promote their - possible - interest in the job description. However, gender diversity can also be promoted and fostered through adapted measures by leaders.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Breidenbach, Aurélie and Mahlow, Caroline and Schreiber, Andreas},
	month = jul,
	year = {2021},
	note = {arXiv:2107.01624 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Software Engineering, K.4.2, K.7.0},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/XVIYDQJG/Breidenbach et al. - 2021 - Implicit Gender Bias in Computer Science -- A Qual.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/5KF8FC5P/2107.html:text/html},
}

@misc{xue_contextual_2022,
	title = {Contextual {Text} {Block} {Detection} towards {Scene} {Text} {Understanding}},
	url = {http://arxiv.org/abs/2207.12955},
	abstract = {Most existing scene text detectors focus on detecting characters or words that only capture partial text messages due to missing contextual information. For a better understanding of text in scenes, it is more desired to detect contextual text blocks (CTBs) which consist of one or multiple integral text units (e.g., characters, words, or phrases) in natural reading order and transmit certain complete text messages. This paper presents contextual text detection, a new setup that detects CTBs for better understanding of texts in scenes. We formulate the new setup by a dual detection task which first detects integral text units and then groups them into a CTB. To this end, we design a novel scene text clustering technique that treats integral text units as tokens and groups them (belonging to the same CTB) into an ordered token sequence. In addition, we create two datasets SCUT-CTW-Context and ReCTS-Context to facilitate future research, where each CTB is well annotated by an ordered sequence of integral text units. Further, we introduce three metrics that measure contextual text detection in local accuracy, continuity, and global accuracy. Extensive experiments show that our method accurately detects CTBs which effectively facilitates downstream tasks such as text classification and translation. The project is available at https://sg-vilab.github.io/publication/xue2022contextual/.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Xue, Chuhui and Huang, Jiaxing and Lu, Shijian and Wang, Changhu and Bai, Song},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12955 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by ECCV2022},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/QPUZHFQA/Xue et al. - 2022 - Contextual Text Block Detection towards Scene Text.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/7MQ9P5SU/2207.html:text/html},
}

@misc{pardos_map_2018,
	title = {A {Map} of {Knowledge}},
	url = {http://arxiv.org/abs/1811.07974},
	abstract = {Knowledge representation has gained in relevance as data from the ubiquitous digitization of behaviors amass and academia and industry seek methods to understand and reason about the information they encode. Success in this pursuit has emerged with data from natural language, where skip-grams and other linear connectionist models of distributed representation have surfaced scrutable relational structures which have also served as artifacts of anthropological interest. Natural language is, however, only a fraction of the big data deluge. Here we show that latent semantic structure, comprised of elements from digital records of our interactions, can be informed by behavioral data and that domain knowledge can be extracted from this structure through visualization and a novel mapping of the literal descriptions of elements onto this behaviorally informed representation. We use the course enrollment behaviors of 124,000 students at a public university to learn vector representations of its courses. From these behaviorally informed representations, a notable 88\% of course attribute information were recovered (e.g., department and division), as well as 40\% of course relationships constructed from prior domain knowledge and evaluated by analogy (e.g., Math 1B is to Math H1B as Physics 7B is to Physics H7B). To aid in interpretation of the learned structure, we create a semantic interpolation, translating course vectors to a bag-of-words of their respective catalog descriptions. We find that the representations learned from enrollments resolved course vectors to a level of semantic fidelity exceeding that of their catalog descriptions, depicting a vector space of high conceptual rationality. We end with a discussion of the possible mechanisms by which this knowledge structure may be informed and its implications for data science.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Pardos, Zachary A. and Nam, Andrew Joo Hun},
	month = nov,
	year = {2018},
	note = {arXiv:1811.07974 [cs]},
	keywords = {Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/QBLJH4K4/Pardos and Nam - 2018 - A Map of Knowledge.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/6JMRA4AW/1811.html:text/html},
}

@misc{wang_enhancing_2022,
	title = {Enhancing {Document}-level {Relation} {Extraction} by {Entity} {Knowledge} {Injection}},
	url = {http://arxiv.org/abs/2207.11433},
	abstract = {Document-level relation extraction (RE) aims to identify the relations between entities throughout an entire document. It needs complex reasoning skills to synthesize various knowledge such as coreferences and commonsense. Large-scale knowledge graphs (KGs) contain a wealth of real-world facts, and can provide valuable knowledge to document-level RE. In this paper, we propose an entity knowledge injection framework to enhance current document-level RE models. Specifically, we introduce coreference distillation to inject coreference knowledge, endowing an RE model with the more general capability of coreference reasoning. We also employ representation reconciliation to inject factual knowledge and aggregate KG representations and document representations into a unified space. The experiments on two benchmark datasets validate the generalization of our entity knowledge injection framework and the consistent improvement to several document-level RE models.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Wang, Xinyi and Wang, Zitao and Sun, Weijian and Hu, Wei},
	month = jul,
	year = {2022},
	note = {arXiv:2207.11433 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted in the 21th International Semantic Web Conference (ISWC 2022)},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/862FNVPF/Wang et al. - 2022 - Enhancing Document-level Relation Extraction by En.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/W8SLS97Z/2207.html:text/html},
}

@misc{wang_person-job_2022,
	title = {Person-job fit estimation from candidate profile and related recruitment history with co-attention neural networks},
	url = {http://arxiv.org/abs/2206.09116},
	abstract = {Existing online recruitment platforms depend on automatic ways of conducting the person-job fit, whose goal is matching appropriate job seekers with job positions. Intuitively, the previous successful recruitment records contain important information, which should be helpful for the current person-job fit. Existing studies on person-job fit, however, mainly focus on calculating the similarity between the candidate resumes and the job postings on the basis of their contents, without taking the recruiters' experience (i.e., historical successful recruitment records) into consideration. In this paper, we propose a novel neural network approach for person-job fit, which estimates person-job fit from candidate profile and related recruitment history with co-attention neural networks (named PJFCANN). Specifically, given a target resume-job post pair, PJFCANN generates local semantic representations through co-attention neural networks and global experience representations via graph neural networks. The final matching degree is calculated by combining these two representations. In this way, the historical successful recruitment records are introduced to enrich the features of resumes and job postings and strengthen the current matching process. Extensive experiments conducted on a large-scale recruitment dataset verify the effectiveness of PJFCANN compared with several state-of-the-art baselines. The codes are released at: https://github.com/CCIIPLab/PJFCANN.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Wang, Ziyang and Wei, Wei and Xu, Chenwei and Xu, Jun and Mao, Xian-Ling},
	month = jun,
	year = {2022},
	note = {arXiv:2206.09116 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/FALCLIJ6/Wang et al. - 2022 - Person-job fit estimation from candidate profile a.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/HUG2KSE6/2206.html:text/html},
}

@inproceedings{gutierrez_augury_2016,
	title = {{AUGURY}: {A} time-series based application for the analysis and forecasting of system and network performance metrics},
	shorttitle = {{AUGURY}},
	url = {http://arxiv.org/abs/1607.08344},
	doi = {10.1109/SYNASC.2016.062},
	abstract = {This paper presents AUGURY, an application for the analysis of monitoring data from computers, servers or cloud infrastructures. The analysis is based on the extraction of patterns and trends from historical data, using elements of time-series analysis. The purpose of AUGURY is to aid a server administrator by forecasting the behaviour and resource usage of specific applications and in presenting a status report in a concise manner. AUGURY provides tools for identifying network traffic congestion and peak usage times, and for making memory usage projections. The application data processing specialises in two tasks: the parametrisation of the memory usage of individual applications and the extraction of the seasonal component from network traffic data. AUGURY uses a different underlying assumption for each of these two tasks. With respect to the memory usage, a limited number of single-valued parameters are assumed to be sufficient to parameterize any application being hosted on the server. Regarding the network traffic data, long-term patterns, such as hourly or daily exist and are being induced by work-time schedules and automatised administrative jobs. In this paper, the implementation of each of the two tasks is presented, tested using locally-generated data, and applied to data from weather forecasting applications hosted on a web server. This data is used to demonstrate the insight that AUGURY can add to the monitoring of server and cloud infrastructures.},
	urldate = {2022-07-28},
	booktitle = {2016 18th {International} {Symposium} on {Symbolic} and {Numeric} {Algorithms} for {Scientific} {Computing} ({SYNASC})},
	author = {Gutierrez, Nicolas and Wiesinger-Widi, Manuela},
	month = sep,
	year = {2016},
	note = {arXiv:1607.08344 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	pages = {351--358},
	annote = {Comment: 8 pages, 9 figures, submitted to SYNASC2016},
	file = {arXiv Fulltext PDF:/Users/abulitibu.tuguluke/Zotero/storage/8VTZ6AVI/Gutierrez and Wiesinger-Widi - 2016 - AUGURY A time-series based application for the an.pdf:application/pdf;arXiv.org Snapshot:/Users/abulitibu.tuguluke/Zotero/storage/Z72247VX/1607.html:text/html},
}
